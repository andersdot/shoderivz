{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating a simple harmonic oscillator and trying to infer the spring constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "import scipy.optimize as so\n",
    "%matplotlib inline\n",
    "import autograd.numpy as np # Thinly-wrapped numpy\n",
    "#import numpy as np  \n",
    "from autograd import grad  \n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import autograph\n",
    "import leapfrog as lf\n",
    "from tensorflow.python.ops import gradients_impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define python functions first to compare with TF to debug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genData(x, v, npoints, std_noise_x, std_noise_v):\n",
    "    noise_x = np.random.normal(0, std_noise_x, len(x))\n",
    "    noise_v = np.random.normal(0, std_noise_v, len(x))\n",
    "    return noise_x + x, noise_v + v\n",
    "\n",
    "def ln_likelihood(theta, data, dt_model):\n",
    "    chi2 = 0\n",
    "    k, x0, v0, *t_obs = theta\n",
    "    x_obs, v_obs, sigma_x, sigma_v = data\n",
    "    x, v, _, _ = leapfrog(x0, v0, np.array(t_obs), potential_and_grad_py, dt_model, k=k)\n",
    "    chi2 += -(v - v_obs)**2 / sigma_v**2 - 2*np.log(sigma_v)\n",
    "    chi2 += -(x - x_obs)**2 / sigma_x**2 - 2*np.log(sigma_x)\n",
    "    return 0.5*chi2.sum()\n",
    "\n",
    "def tf_log_like(x0, v0, k, t_obs, step_size, data, name='loglikelihood'):\n",
    "    with tf.name_scope(name):\n",
    "        chi2 = 0\n",
    "        x_obs, v_obs, sigma_x, sigma_v = data\n",
    "        x, v = lf.leapfrog(x0, v0, k, t_obs, step_size, name='leapfrog')\n",
    "        chi2 += -(v - v_obs)**2 / sigma_v**2 - 2*np.log(sigma_v)\n",
    "        chi2 += -(x - x_obs)**2 / sigma_x**2 - 2*np.log(sigma_x)\n",
    "        return 0.5*tf.reduce_sum(chi2)\n",
    "\n",
    "def nll_python(*args):\n",
    "    return -ln_likelihood(*args)\n",
    "\n",
    "def potential_and_grad_py(position, k=1.0):\n",
    "    #function that returns the potential and it's gradient at a given position\n",
    "    return 0.5 * k * position**2, k*position\n",
    "\n",
    "def leap(position, momentum, grad, potential_and_grad, step_size, k=1.0):\n",
    "    momentum -= 0.5 * step_size * grad\n",
    "    position += step_size * momentum\n",
    "    potential, grad = potential_and_grad_py(position, k=k)\n",
    "    momentum -= 0.5 * step_size * grad\n",
    "    return position, momentum, potential, grad\n",
    "\n",
    "def leapfrog(x0, v0, t_obs, potential_and_grad_py, dt, k=np.float64(1.0)):\n",
    "    #function that takes initial conditions that takes us to the next position \n",
    "    x = [] \n",
    "    v = [] \n",
    "    t = [] \n",
    "    grads = []\n",
    "    time = []\n",
    "    \n",
    "    tprime = 0.0\n",
    "    xprime = x0\n",
    "    vprime = v0\n",
    "\n",
    "    pot, grad = potential_and_grad_py(xprime, k=k)\n",
    "    for to in t_obs:\n",
    "\n",
    "        while tprime + dt < to:\n",
    "            xprime, vprime, pot, grad = leap(xprime, vprime, grad, potential_and_grad_py, dt, k=k)\n",
    "            tprime = tprime + dt    \n",
    "        dt_tiny = to - tprime\n",
    "        xsave, vsave, potsave, gradsave = leap(xprime, vprime, grad, potential_and_grad_py, dt_tiny, k=k)\n",
    "        tsave = tprime + dt_tiny\n",
    "        #print(xsave, vsave, tsave, potsave, gradsave)\n",
    "        x.append(xsave)\n",
    "        v.append(vsave)\n",
    "        t.append(tsave)\n",
    "        grads.append(grad)\n",
    "        time.append(tprime)\n",
    "        #print(x, v)\n",
    "    return np.array(x), np.array(v), np.array(grads), np.array(time) #, np.array(t)\n",
    "\n",
    "def finite_difference_grads(k_true, x0_true, \n",
    "                            v0_true, t_obs_true, \n",
    "                            epsilon):\n",
    "    grads = []\n",
    "    \n",
    "    k = k_true\n",
    "    x0 = x0_true\n",
    "    v0 = v0_true\n",
    "    t0 = t_obs_true\n",
    "    k = k_true + epsilon/2.\n",
    "    p0_test = [k, x0, v0] + (t0).tolist()\n",
    "    kp = nll_python(p0_test, data, s_size)\n",
    "    k = k_true - epsilon/2.\n",
    "    p0_test = [k, x0, v0] + (t0).tolist()\n",
    "    km = nll_python(p0_test, data, s_size)\n",
    "    k = k_true\n",
    "\n",
    "    grads.append((kp - km)/epsilon)\n",
    "\n",
    "\n",
    "    x0 = x0_true + epsilon/2.\n",
    "    p0_test = [k, x0, v0] + (t0).tolist()\n",
    "    xp = nll_python(p0_test, data, s_size)\n",
    "    x0 = x0_true - epsilon/2.\n",
    "    p0_test = [k, x0, v0] + (t0).tolist()\n",
    "    xm = nll_python(p0_test, data, s_size)\n",
    "    x0 = x0_true\n",
    "\n",
    "    grads.append((xp - xm)/epsilon)\n",
    "\n",
    "    v0 = v0_true + epsilon/2.\n",
    "    p0_test = [k, x0, v0] + (t0).tolist()\n",
    "    vp = nll_python(p0_test, data, s_size)\n",
    "    v0 = v0_true - epsilon/2.\n",
    "    p0_test = [k, x0, v0] + (t0).tolist()\n",
    "    vm = nll_python(p0_test, data, s_size)\n",
    "    v0 = v0_true\n",
    "\n",
    "    grads.append((vp - vm)/epsilon)\n",
    "\n",
    "    for i, t in enumerate(t0):\n",
    "        t_obs_true[i] = t + epsilon/2.\n",
    "        p0_test = [k, x0, v0] + (t0).tolist()\n",
    "        tp = nll_python(p0_test, data, s_size)\n",
    "        t_obs_true[i] = t - epsilon/2.\n",
    "        p0_test = [k, x0, v0] + (t0).tolist()\n",
    "        tm = nll_python(p0_test, data, s_size)\n",
    "        t_obs_true[i] = t\n",
    "\n",
    "        grads.append((tp - tm)/epsilon)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some true values and initial guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define true parameter values we will add noise to later\n",
    "x0_true   = np.float64(10.)\n",
    "v0_true   = np.float64(10.)\n",
    "k_true    = np.float64(3.)\n",
    "\n",
    "#define step size of each leap and number of shos\n",
    "s_size = np.float64(0.01)      #resolution of each leap\n",
    "n_shos = 1            #number of simple harmonic oscillators \n",
    "\n",
    "#define true observed times\n",
    "max_time  = np.float64(10.)\n",
    "nobspoints = 10\n",
    "t_obs_true = np.random.uniform(0, max_time, nobspoints)\n",
    "t_obs_true.sort()\n",
    "\n",
    "#define noise properties \n",
    "std_noise_x = 1.0\n",
    "std_noise_v = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fake data and model predictions using python implementation (which I trust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate true values and noisify them\n",
    "x_true, v_true, grad_true, time_steps_true = leapfrog(x0_true, v0_true, t_obs_true, potential_and_grad_py, s_size, k=k_true)\n",
    "x_obs, v_obs = genData(x_true, v_true, nobspoints, std_noise_x, std_noise_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data   = [x_obs, v_obs, std_noise_x, std_noise_v]\n",
    "\n",
    "k_guess = k_true  + np.random.normal(0, k_true)\n",
    "x0_guess = x0_true + np.random.normal(0, std_noise_x)\n",
    "v0_guess = v0_true + np.random.normal(0, std_noise_v)\n",
    "t0_guess = t_obs_true #+ np.random.normal(0, 1., len(t_obs_true))\n",
    "t0_guess.sort()\n",
    "p0_guess = [k_guess, \n",
    "          x0_guess, \n",
    "          v0_guess] + (t0_guess).tolist()\n",
    "\n",
    "#for plotting purposes\n",
    "t_compare = np.linspace(0, max_time, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate log likelihood of python model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26 ms ± 10.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#calculate log likelihood of initial guesses using python leapfrog model\n",
    "theta = [k_guess, x0_guess, v0_guess] + t0_guess.tolist()\n",
    "data = [x_obs, v_obs, std_noise_x, std_noise_v]\n",
    "%timeit python_loglikelihood = ln_likelihood(theta, data, s_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate autograd gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define autograd gradient function\n",
    "grad_ln_like = grad(nll_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 ms ± 5.32 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit grads_autograd = grad_ln_like(p0_guess, data, s_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate finite difference gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.5 ms ± 651 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "%timeit grads_finite_difference = finite_difference_grads(k_guess, x0_guess, v0_guess, t0_guess, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try with TensorFlow\n",
    "## Define Tensorflow variables and Tensorflow models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define log likelihood model\n",
    "#ll = tf_log_like(x, v, x_obs_tf, v_obs_tf, std_noise_x_tf, std_noise_v_tf)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "data = [x_obs, v_obs, std_noise_x, std_noise_v]\n",
    "\n",
    "#turn initial parameter guesses into tensorflow tensors so tensorflow can take gradients of them\n",
    "\n",
    "x0_tf    = tf.Variable(tf.constant(x0_guess), name = \"x0\"   , dtype=np.float64)\n",
    "v0_tf    = tf.Variable(tf.constant(v0_guess), name = \"v0\"   , dtype=np.float64)\n",
    "k_tf     = tf.Variable(tf.constant(k_guess),  name = \"k\"    , dtype=np.float64)\n",
    "t_obs_tf = tf.Variable(tf.constant(t0_guess), name = \"t_obs\", dtype=np.float64)    \n",
    "\n",
    "#define tensorflow models\n",
    "#returns the modeled x and v values \n",
    "model = lf.leapfrog(x0_tf, v0_tf, k_tf, t_obs_tf, s_size, name='leapfrog')\n",
    "#returns the negative log likelihood of the parameters\n",
    "nll = -tf_log_like(x0_tf, v0_tf, k_tf, t_obs_tf, s_size, data, name='negativeloglike')\n",
    "#returns the likelihood of the parameters\n",
    "ll = tf_log_like(x0_tf, v0_tf, k_tf, t_obs_tf, s_size, data, name='loglikelihood')\n",
    "\n",
    "#returns the gradients of the negative log likelihood\n",
    "gradients = tf.gradients(nll, [k_tf, x0_tf, v0_tf, t_obs_tf]) #, current_v, k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Tensorflow models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.2 ms ± 2.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "149 ms ± 2.77 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "session = tf.get_default_session()\n",
    "if session is None:\n",
    "    session = tf.InteractiveSession()\n",
    "#initialize all variables that are defined\n",
    "session.run(tf.global_variables_initializer())\n",
    "#run model to get x, v for debugging purposes in plots below\n",
    "x, v = session.run(model)\n",
    "#writer = tf.summary.FileWriter(\"/tmp/leapfrog/model\")\n",
    "#writer.add_graph(session.graph)\n",
    "#compute likelihood of model parameters\n",
    "tensorflow_loglikelihood = session.run(ll)\n",
    "%timeit session.run(ll)\n",
    "#writer = tf.summary.FileWriter(\"/tmp/leapfrog/loglike\")\n",
    "#writer.add_graph(session.graph)\n",
    "#compute the gradients of the negative log likelihood with respect to all the parameters \n",
    "grads_tensorflow = session.run(gradients)\n",
    "%timeit session.run(gradients)\n",
    "#writer = tf.summary.FileWriter(\"/tmp/leapfrog/negloglike\")\n",
    "#writer.add_graph(session.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shoop import sho_integrate\n",
    "from exoplanet.interp import interp1d, CubicInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 µs ± 2.25 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "294 µs ± 3.06 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfEkqhMGXpEIYfEkqxP8BwfXwN1jrrZkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c2a377ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "T = tf.float64\n",
    "\n",
    "N = int(max_time / s_size + 1)\n",
    "\n",
    "t, x, v, a = sho_integrate(x0_tf, v0_tf, k_tf, N, s_size)\n",
    "#interpx = CubicInterpolator(t, x, dtype=T)\n",
    "#interpv = CubicInterpolator(t, v, dtype=T)\n",
    "#x_in = interpx.evaluate(t_obs_tf)\n",
    "#v_in = interpv.evaluate(t_obs_tf)\n",
    "x_in = interp1d(t_obs_tf, t, x)\n",
    "v_in = interp1d(t_obs_tf, t, v)\n",
    "\n",
    "\n",
    "loglike  = -0.5 * tf.reduce_sum(tf.square((x_obs - x_in) / std_noise_x)) - np.log(std_noise_x)\n",
    "loglike += -0.5 * tf.reduce_sum(tf.square((v_obs - v_in) / std_noise_v)) - np.log(std_noise_v)\n",
    "\n",
    "plt.plot(t_obs_tf.eval(), x_obs, \".\")\n",
    "\n",
    "var = [k_tf, x0_tf, v0_tf]\n",
    "grad = tf.gradients(-loglike, var)\n",
    "cpp_loglikelihood = session.run(loglike)\n",
    "%timeit session.run(loglike)\n",
    "grads_cpp = session.run(grad)\n",
    "%timeit session.run(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now plot some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-206.48235333508873 -206.48631941943646\n",
      "[-910.5718737215423, -23.761846790415465, 40.662344525910626]\n",
      "[-910.5895475716925, -23.758126710986428, 40.66525166084409, IndexedSlicesValue(values=array([   0.        , -146.40850364, -137.81835233, -266.30382275,\n",
      "       -212.0290461 ,  -50.89309726, -120.27818885, -104.04905975,\n",
      "        -33.58591233, -105.84012042,  -23.90594088]), indices=array([0, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0], dtype=int32), dense_shape=array([10], dtype=int32))]\n"
     ]
    }
   ],
   "source": [
    "print(cpp_loglikelihood, tensorflow_loglikelihood)\n",
    "print(grads_cpp)\n",
    "print(grads_tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-833bb55aee07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "print(ll, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate values using python leapfrog from initial guesses to plot\n",
    "plot_t_obs_values = np.linspace(0, max_time, 1000)\n",
    "xleap_plot, vleap_plot, gradleap_plot, timeleap_plot = leapfrog(x0_guess,\n",
    "                                            v0_guess,\n",
    "                                            plot_t_obs_values,\n",
    "                                            potential_and_grad_py,\n",
    "                                            s_size,\n",
    "                                            k=k_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate values using python leapfrog from initial guesses to compare with tf model\n",
    "xleap, vleap, gradleap, timeleap = leapfrog(x0_guess,\n",
    "                                            v0_guess,\n",
    "                                            t0_guess,\n",
    "                                            potential_and_grad_py,\n",
    "                                            s_size,\n",
    "                                            k=k_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#now plot some stuff\n",
    "fig, ax = plt.subplots(1,2, figsize=(5, 2.5))\n",
    "ax[0].plot(t0_guess, x - xleap, 'o')\n",
    "#print(session.run(x))\n",
    "ax[0].set_xlabel('t')\n",
    "ax[0].set_ylabel('$\\Delta x$')\n",
    "ax[1].set_xlabel('t')\n",
    "ax[1].set_ylabel('$\\Delta y$')\n",
    "ax[1].plot(t0_guess, v - vleap, 'o')\n",
    "fig.suptitle('Differences Between Python and Tensorflow Model')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(5, 2.5))\n",
    "ax[0].plot(t0_guess, xleap, 'o', label='python')\n",
    "ax[0].plot(t0_guess, x, 'o', label='tf')\n",
    "ax[0].plot(plot_t_obs_values, xleap_plot, 'k-')\n",
    "ax[0].set_xlabel('t')\n",
    "ax[0].set_ylabel('x')\n",
    "ax[1].set_xlabel('t')\n",
    "ax[1].set_ylabel('y')\n",
    "ax[1].plot(t0_guess, vleap, 'o')\n",
    "ax[1].plot(t0_guess, v, 'o')\n",
    "ax[1].plot(plot_t_obs_values, vleap_plot, 'k-')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the gradients to show Tensorflow, Autograd, and Finite Differences generate the same values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python loglikelihood is: {0}'.format(python_loglikelihood))\n",
    "print('Tensorflow loglikelihood is: {0}'.format(tensorflow_loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['k', 'x0', 'v0'] + ['tobs']*nobspoints\n",
    "print('Gradients of Negative Log Likelihood')\n",
    "print('  TensorFlow  Autograd  FiniteDifference ')\n",
    "for tg, ag, fg, k in zip(grads_tensorflow[0:3], grads_autograd[0:3], grads_finite_difference[0:3], keys[0:3]):\n",
    "    print('{0}  {1:0.3f}   {2:0.3f}   {3:0.3f}'.format(k, tg, ag, fg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradients of Negative Log Likelihood')\n",
    "print('  TensorFlow  Autograd  FiniteDifference ')\n",
    "for tg, ag, fg, k in zip(grads_tensorflow[3].values[::-1][:-1], grads_autograd[3:], grads_finite_difference[3:], keys[3:]):\n",
    "    print('{0}   {1:0.3f}    {2:0.3f}    {3:0.3f}'.format(k, tg, ag, fg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit grads_autograd = grad_ln_like(p0_guess, data, s_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "%timeit grads_finite_difference = finite_difference_grads(k_guess, x0_guess, v0_guess, t0_guess, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit session.run(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit session.run(gradients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
