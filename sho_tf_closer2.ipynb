{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating a simple harmonic oscillator and trying to infer the spring constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "import scipy.optimize as so\n",
    "%matplotlib inline\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd import grad  \n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import autograph\n",
    "import tfleapfrog_copy2 as tflf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define python functions first to compare with TF to debug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genData(x, v, npoints, std_noise_x, std_noise_v):\n",
    "    noise_x = np.random.normal(0, std_noise_x, len(x))\n",
    "    noise_v = np.random.normal(0, std_noise_v, len(x))\n",
    "    return noise_x + x, noise_v + v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_likelihood(theta, data, dt_model):\n",
    "    chi2 = 0\n",
    "    k, x0, v0, *t_obs = theta\n",
    "    x_obs, v_obs, sigma_x, sigma_v = data\n",
    "    x, v, _, _ = leapfrog(x0, v0, np.array(t_obs), potential_and_grad_py, dt_model, k=k)\n",
    "    chi2 += -(v - v_obs)**2 / sigma_v**2 - 2*np.log(sigma_v)\n",
    "    chi2 += -(x - x_obs)**2 / sigma_x**2 - 2*np.log(sigma_x)\n",
    "    return 0.5*chi2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def potential_and_grad_py(position, k=1.0):\n",
    "    #function that returns the potential and it's gradient at a given position\n",
    "    return 0.5 * k * position**2, k*position\n",
    "\n",
    "def leap(position, momentum, grad, potential_and_grad, step_size, k=1.0):\n",
    "    momentum -= 0.5 * step_size * grad\n",
    "    position += step_size * momentum\n",
    "    potential, grad = potential_and_grad_py(position, k=k)\n",
    "    momentum -= 0.5 * step_size * grad\n",
    "    return position, momentum, potential, grad\n",
    "\n",
    "def leapfrog(x0, v0, t_obs, potential_and_grad_py, dt, k=np.float64(1.0)):\n",
    "    #function that takes initial conditions that takes us to the next position \n",
    "    x = [] \n",
    "    v = [] \n",
    "    t = [] \n",
    "    grads = []\n",
    "    time = []\n",
    "    \n",
    "    tprime = np.float64(0.0)\n",
    "    xprime = np.float64(x0)\n",
    "    vprime = np.float64(v0)\n",
    "    pot, grad = potential_and_grad_py(xprime, k=k)\n",
    "    for to in t_obs:\n",
    "\n",
    "        while tprime + dt < to:\n",
    "            xprime, vprime, pot, grad = leap(xprime, vprime, grad, potential_and_grad_py, dt, k=k)\n",
    "            tprime = tprime + dt    \n",
    "        dt_tiny = to - tprime\n",
    "        xsave, vsave, potsave, gradsave = leap(xprime, vprime, grad, potential_and_grad_py, dt_tiny, k=k)\n",
    "        tsave = tprime + dt_tiny\n",
    "        #print(xsave, vsave, tsave, potsave, gradsave)\n",
    "        x.append(xsave.copy())\n",
    "        v.append(vsave.copy())\n",
    "        t.append(tsave.copy())\n",
    "        grads.append(grad)\n",
    "        time.append(tprime)\n",
    "        #print(x, v)\n",
    "    return np.array(x), np.array(v), np.array(grads), np.array(time) #, np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some true values and initial guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0_true   = 100.\n",
    "v0_true   = 100.\n",
    "k_true    = 10.\n",
    "\n",
    "#define step size of each leap and number of shos\n",
    "s_size = np.float64(0.01)      #resolution of each leap\n",
    "n_shos = 1            #number of simple harmonic oscillators \n",
    "\n",
    "#set spring constant initial guess\n",
    "current_k_value = np.float64(10.0)\n",
    "\n",
    "#generate initial velocities and momenta guesses\n",
    "current_x_value = np.float64(10.0) #np.float64(np.random.randn(n_shos))\n",
    "current_v_value = np.float64(4.0) #np.float64(np.random.randn(n_shos))\n",
    "\n",
    "max_time  = np.float64(10.)\n",
    "nobspoints = 10\n",
    "#t_obs_true = np.linspace(0, max_time, nobspoints) \n",
    "t_obs_true = np.random.uniform(0, max_time, nobspoints)\n",
    "t_obs_true.sort()\n",
    "#generate initial times to model SHO, for now set equal to the true times\n",
    "current_t_obs_values = t_obs_true #np.random.uniform(0, max_time, nobspoints)#t_obs_init.sort()\n",
    "\n",
    "std_noise_x = 10.0\n",
    "std_noise_v = 10.0\n",
    "plot_xerr = np.zeros(nobspoints) + std_noise_x\n",
    "plot_yerr = np.zeros(nobspoints) + std_noise_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fake data and model predictions using python implementation (which I trust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate true values and noisify them\n",
    "x_true, v_true, grad_true, time_steps_true = leapfrog(x0_true, v0_true, t_obs_true, potential_and_grad_py, s_size, k=k_true)\n",
    "x_obs, v_obs = genData(x_true, v_true, nobspoints, std_noise_x, std_noise_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate values using python leapfrog from initial guesses to compare with tf model\n",
    "xleap, vleap, gradleap, timeleap = leapfrog(current_x_value,\n",
    "                                            current_v_value,\n",
    "                                            current_t_obs_values,\n",
    "                                            potential_and_grad_py,\n",
    "                                            s_size,\n",
    "                                            k=current_k_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2081.361073961145\n"
     ]
    }
   ],
   "source": [
    "#calculate log likelihood of initial guesses using python leapfrog model\n",
    "theta = [current_k_value, current_x_value, current_v_value] +  current_t_obs_values.tolist()\n",
    "data = [x_obs, v_obs, std_noise_x, std_noise_v]\n",
    "print(ln_likelihood(theta, data, s_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def evolve(params, t):\n",
    "    position_eval, momentum_eval, k, grad_eval, potential_eval, t_previous, x_eval, v_eval = params\n",
    "    print(t, t_previous)\n",
    "    deltat = t - t_previous\n",
    "    print(deltat)\n",
    "    new_p_eval, new_m_eval, new_p_eval, new_g_eval, new_x_eval, new_v_eval, new_time_step_eval = session.run(\n",
    "      [position_model, momentum_model, potential_model, grad_model, x_model, v_model, time_on_step_model],\n",
    "      feed_dict = {position: position_eval, momentum: momentum_eval, k: k, dt: deltat})#, grad:grad_val})\n",
    "    new_t_previous = t_previous + new_time_step_eval\n",
    "\n",
    "    params = [new_p_eval, new_m_eval, k, new_g_eval, new_p_eval, new_t_previous, new_x_val, new_v_val]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Main_1/leapfrog_integrator/while/Exit_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/while/Exit_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/mul_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/mul_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/add:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/sub_1:0\", shape=(), dtype=float64)\n",
      "Tensor(\"Main_1/leapfrog_integrator/while/Exit_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/while/Exit_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/mul_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/mul_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/add:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/sub_1:0\", shape=(), dtype=float64)\n",
      "Tensor(\"Main_1/leapfrog_integrator/while/Exit_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/while/Exit_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/mul_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/mul_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/add:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/sub_1:0\", shape=(), dtype=float64)\n",
      "Tensor(\"Main_1/leapfrog_integrator/while/Exit_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/while/Exit_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/mul_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/mul_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/add:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/sub_1:0\", shape=(), dtype=float64)\n",
      "Tensor(\"Main_1/leapfrog_integrator/while/Exit_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/while/Exit_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/mul_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/mul_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/add:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/sub_1:0\", shape=(), dtype=float64)\n",
      "Tensor(\"Main_1/leapfrog_integrator/while/Exit_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/while/Exit_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/mul_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/mul_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/add:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/sub_1:0\", shape=(), dtype=float64)\n",
      "Tensor(\"Main_1/leapfrog_integrator/while/Exit_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/while/Exit_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/mul_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/mul_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/add:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/sub_1:0\", shape=(), dtype=float64)\n",
      "Tensor(\"Main_1/leapfrog_integrator/while/Exit_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/while/Exit_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/mul_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/mul_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/add:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/sub_1:0\", shape=(), dtype=float64)\n",
      "Tensor(\"Main_1/leapfrog_integrator/while/Exit_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/while/Exit_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/mul_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/mul_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/add:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/sub_1:0\", shape=(), dtype=float64)\n",
      "Tensor(\"Main_1/leapfrog_integrator/while/Exit_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/while/Exit_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/mul_2:0\", shape=(), dtype=float64) Tensor(\"Main_1/mul_3:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/add:0\", shape=(), dtype=float64) Tensor(\"Main_1/leapfrog_integrator/leapfrog_step/sub_1:0\", shape=(), dtype=float64)\n",
      "['Main/current_x:0', 'Main/current_v:0', 'Main/current_grad:0', 'Main/current_x_1:0', 'Main/current_v_1:0', 'Main/k:0', 'Main/t_obs:0', 'Main/t_previous:0', 'Main/t_current:0', 'Main/dt:0', 'Main/x:0', 'Main/v:0', 'Main/nobs:0']\n",
      "The log likelihood computed using tensorflow: -2515.3611652773166\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1425152326af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m         print(\"The log likelihood computed using tensorflow: {0}\"\n\u001b[1;32m    173\u001b[0m               .format(session.run(ll)))\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mgrad_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m#print(\"The gradients are: {0}\".format(session.run(gradients)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1085\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \"\"\"\n\u001b[1;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \"\"\"\n\u001b[1;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n\u001b[0;32m--> 242\u001b[0;31m                                                                  type(fetch)))\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "def potential_and_grad(position, k=np.float64(1.0)):\n",
    "    #function that returns the potential and it's gradient at a given position\n",
    "    return 0.5 * k * tf.square(position), k * position\n",
    "\n",
    "def tf_log_like(x0_input, v0_input, k_input, t_obs_input, obsx, obsv, sigmax, sigmav):\n",
    "    with tf.variable_scope('Main', reuse=tf.AUTO_REUSE): \n",
    "        #assign parameters to model tensors\n",
    "        tf.assign(current_x, x0_input)\n",
    "        tf.assign(current_v, v0_input)\n",
    "        tf.assign(k, k_input)\n",
    "        tf.assign(t_obs, t_obs_input)\n",
    "        tf.assign(t_previous, tf.constant(np.float64(0.0)))\n",
    "        #calculate potential at initial position\n",
    "        current_potential_value, current_grad_value = potential_and_grad(current_x, k=k)\n",
    "        tf.assign(current_grad, current_grad_value)\n",
    "\n",
    "        #loop through observations times and calculate x, v\n",
    "        for i in range(10):\n",
    "            #calculate time to evolve for current iteration\n",
    "            tf.assign(dt, t_obs[i] - t_previous)\n",
    "\n",
    "            #run the tf model\n",
    "            new_x, new_v, new_grad, new_t, x_value, v_value = model_leap\n",
    "            print(new_x, new_v, new_grad, new_t, x_value, v_value)\n",
    "            #update tf tensors with new values and save x, v\n",
    "            tf.assign(x[i], x_value)\n",
    "            tf.assign(v[i], v_value)\n",
    "            tf.assign(current_x, new_x)\n",
    "            tf.assign(current_v, new_v)\n",
    "            tf.assign(current_grad, new_grad)\n",
    "            tf.assign(t_previous, new_t + t_previous)\n",
    "\n",
    "            #save a couple other things for debugging\n",
    "            #time_step[i] = t_previous\n",
    "            #grads[i] = new_grad\n",
    "\n",
    "        #calculate loglikelihood for these model parameters\n",
    "        chi2 = np.float64(0.0)\n",
    "        chi2 += (v - obsv)**2 / sigmav**2 + 2*tf.log(sigmav)\n",
    "        chi2 += (x - obsx)**2 / sigmax**2 + 2*tf.log(sigmax)\n",
    "\n",
    "    return -0.5*tf.reduce_sum(chi2)\n",
    "\n",
    "\"\"\"\n",
    "def tf_log_like(x0_input, v0_input, k_input, t_obs_input, obsx, obsv, sigmax, sigmav):\n",
    "    with tf.Session() as session:\n",
    "        with tf.variable_scope('Main', reuse=tf.AUTO_REUSE): \n",
    "            #assign parameters to model tensors\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.assign(current_x, x0_input))\n",
    "            session.run(tf.assign(current_v, v0_input))\n",
    "            session.run(tf.assign(k, k_input))\n",
    "            session.run(tf.assign(t_obs, t_obs_input))\n",
    "            session.run(tf.assign(t_previous, tf.constant(np.float64(0.0))))\n",
    "            #calculate potential at initial position\n",
    "            current_potential_value, current_grad_value = session.run([potential_model, grad_model])\n",
    "            tf.assign(current_grad, np.float64(current_grad_value))\n",
    "\n",
    "            #loop through observations times and calculate x, v\n",
    "            for i in range(nobspoints):\n",
    "                #calculate time to evolve for current iteration\n",
    "                session.run(tf.assign(dt, t_obs[i] - t_previous))\n",
    "                \n",
    "                #run the tf model\n",
    "                new_x, new_v, new_grad, new_t, x_value, v_value = model_leap\n",
    "\n",
    "                #update tf tensors with new values and save x, v\n",
    "                session.run(tf.assign(x[i], x_value))\n",
    "                session.run(tf.assign(v[i], v_value))\n",
    "                session.run(tf.assign(current_x, new_x))\n",
    "                session.run(tf.assign(current_v, new_v))\n",
    "                session.run(tf.assign(current_grad, new_grad))\n",
    "                session.run(tf.assign(t_previous, new_t + t_previous))\n",
    "                \n",
    "                #save a couple other things for debugging\n",
    "                time_step[i] = session.run(t_previous)\n",
    "                grads[i] = new_grad\n",
    "            \n",
    "            #calculate loglikelihood for these model parameters\n",
    "            chi2 = np.float64(0.0)\n",
    "            chi2 += (v - obsv)**2 / sigmav**2 + 2*tf.log(sigmav)\n",
    "            chi2 += (x - obsx)**2 / sigmax**2 + 2*tf.log(sigmax)\n",
    "\n",
    "    return -0.5*tf.reduce_sum(chi2)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#=============================================================\n",
    "#=======  SETTING UP VARIABLES ===============================\n",
    "#=============================================================\n",
    "\n",
    "#the time resolution of the integrator\n",
    "step_size = tf.constant(s_size, dtype=np.float64, name='step_size') #tf.placeholder(np.float32, name='step_size')\n",
    "i         = tf.constant(np.float64(0.0), name='i')\n",
    "\n",
    "#turn fake data into tensorflow tensors \n",
    "x_obs_tf = tf.constant(x_obs, dtype=np.float64)\n",
    "v_obs_tf = tf.constant(v_obs, dtype=np.float64)\n",
    "std_noise_x_tf = tf.constant(std_noise_x, dtype=np.float64)\n",
    "std_noise_v_tf = tf.constant(std_noise_v, dtype=np.float64)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"Main\"):\n",
    "\n",
    "    #create placeholders for initial position and momentum, spring constant, and obs times to define model\n",
    "    #these are the model parameters the optimize will find\n",
    "    current_x    = tf.Variable(tf.constant(current_x_value), name = \"current_x\", dtype=tf.float64)\n",
    "    current_v    = tf.Variable(tf.constant(current_v_value), name = \"current_v\", dtype=np.float64)\n",
    "    current_grad = tf.Variable(tf.constant(np.float64(0.0)), name = \"current_grad\", dtype=np.float64)\n",
    "    \n",
    "    x0           = tf.Variable(tf.constant(current_x_value), name = \"current_x\", dtype=tf.float64)\n",
    "    v0           = tf.Variable(tf.constant(current_v_value), name = \"current_v\", dtype=np.float64)\n",
    "    k            = tf.Variable(tf.constant(current_k_value), name = \"k\"        , dtype=np.float64)\n",
    "    t_obs        = tf.Variable(tf.constant(current_t_obs_values), name = \"t_obs\"   , dtype=np.float64)    \n",
    "\n",
    "    #the time of the previous observation to help us calculate dt\n",
    "    t_previous = tf.Variable(tf.constant(np.float64(0.0)), name = 't_previous', dtype=tf.float64)\n",
    "    t_current  = tf.Variable(tf.constant(np.float64(1.0)), name = 't_current',  dtype=tf.float64)\n",
    "    #the time between the last observation time and the next\n",
    "    #so how long the current iteration should integrate to get to the next observation\n",
    "    #I'm a bit confused about when to use a placeholder and when to use a variable\n",
    "    #this is something I will always feed into the model and will change with each iteration\n",
    "    dt       = tf.Variable(tf.constant(np.float64(1.0)), name = \"dt\"        , dtype=np.float64)\n",
    "\n",
    "\n",
    "    #the modeled observed positions and velocities compare with the actual observed positions and velocities for the loss function \n",
    "    x         = tf.Variable(tf.zeros(nobspoints, dtype=np.float64), name = \"x\", dtype=tf.float64)\n",
    "    v         = tf.Variable(tf.zeros(nobspoints, dtype=np.float64), name = \"v\", dtype=tf.float64)\n",
    "    nobs      = tf.Variable(tf.constant(nobspoints), name='nobs')\n",
    "\n",
    "#generate arrays to save values from model for debugging purposes \n",
    "grads     = np.zeros([nobspoints, n_shos])\n",
    "time_step = np.zeros([nobspoints, n_shos])\n",
    "\n",
    "\n",
    "\n",
    "#=============================================================\n",
    "#=======  DEFINING MODELS ====================================\n",
    "#=============================================================\n",
    "\n",
    "with tf.variable_scope('Main', reuse=tf.AUTO_REUSE):\n",
    "\n",
    "    #define potential model\n",
    "    potential_model, grad_model = potential_and_grad(current_x, k=k)\n",
    "\n",
    "    #define model, leapfrog_integrator is from hmc tensorflow stuff\n",
    "    model_leap = tflf.leapfrog_integrator(step_size, dt, \n",
    "                                      current_x, current_v, \n",
    "                                      potential_and_grad, \n",
    "                                      current_grad, t_previous, k=k)\n",
    "\n",
    "    #define log likelihood model\n",
    "    #ll = tf_log_like(x, v, x_obs_tf, v_obs_tf, std_noise_x_tf, std_noise_v_tf)\n",
    "    ll = tf_log_like(x0, v0, k, t_obs, x_obs_tf, v_obs_tf, std_noise_x_tf, std_noise_v_tf)\n",
    "\n",
    "    #define gradients \n",
    "    gradients = tf.gradients(ll, [x0, v0]) #, current_v, k])\n",
    "\n",
    "#=============================================================\n",
    "#=======  RUN TENSORFLOW =====================================\n",
    "#=============================================================\n",
    "\n",
    "with tf.Session() as session:\n",
    "    with tf.variable_scope('Main', reuse=tf.AUTO_REUSE):  \n",
    "        session.run(tf.global_variables_initializer())\n",
    "        print([v.name for v in tf.all_variables()])\n",
    "        # This step is needed to set up the variables.\n",
    "\n",
    "\n",
    "        print(\"The log likelihood computed using tensorflow: {0}\"\n",
    "              .format(session.run(ll)))\n",
    "        grad_tf = session.run(gradients)\n",
    "        #print(\"The gradients are: {0}\".format(session.run(gradients)))\n",
    "        \n",
    "        fig, ax = plt.subplots(1,4, figsize=(10, 2.5))\n",
    "        ax[0].plot(current_t_obs_values, session.run(x) - xleap, 'o')\n",
    "        #print(session.run(x))\n",
    "        ax[0].set_xlabel('t')\n",
    "        ax[0].set_ylabel('$\\Delta x$')\n",
    "        ax[1].set_xlabel('t')\n",
    "        ax[1].set_ylabel('$\\Delta y$')\n",
    "        ax[1].plot(current_t_obs_values, session.run(v) - vleap, 'o')\n",
    "        ax[2].set_xlabel('t')\n",
    "        ax[2].set_ylabel('$\\Delta \\Phi$')\n",
    "        ax[2].plot(current_t_obs_values, grads.T[0] - gradleap.T, 'o')\n",
    "        ax[3].set_xlabel('t')\n",
    "        ax[3].set_ylabel('$\\Delta t$')\n",
    "        ax[3].plot(current_t_obs_values, time_step.T[0] - timeleap.T, 'o')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        fig, ax = plt.subplots(1,4, figsize=(10, 2.5))\n",
    "        ax[0].plot(current_t_obs_values, xleap, 'o')\n",
    "        ax[0].plot(current_t_obs_values, session.run(x), 'o')\n",
    "        ax[0].set_xlabel('t')\n",
    "        ax[0].set_ylabel('x')\n",
    "        ax[1].set_xlabel('t')\n",
    "        ax[1].set_ylabel('y')\n",
    "        ax[1].plot(current_t_obs_values, vleap, 'o')\n",
    "        ax[1].plot(current_t_obs_values, session.run(v), 'o')\n",
    "        \n",
    "ax[2].set_xlabel('t')\n",
    "ax[2].set_ylabel('gradient of potential')\n",
    "ax[2].plot(current_t_obs_values, gradleap, 'o')\n",
    "ax[2].plot(current_t_obs_values, grads, 'o')\n",
    "ax[3].set_xlabel('t')\n",
    "ax[3].set_ylabel('time at step')\n",
    "ax[3].plot(current_t_obs_values, time_step, 'o')\n",
    "ax[3].plot(current_t_obs_values, timeleap, 'o')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xleap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def scoping(fn, scope1, scope2, vals):\n",
    "    with fn(scope1):\n",
    "        a = tf.Variable(vals[0], name='a')\n",
    "        b = tf.get_variable('b', initializer=vals[1])\n",
    "        c = tf.constant(vals[2], name='c')\n",
    "\n",
    "        with fn(scope2):\n",
    "            d = tf.add(a * b, c, name='res')\n",
    "\n",
    "        print('\\n  '.join([scope1, a.name, b.name, c.name, d.name]), '\\n')\n",
    "    return d\n",
    "\n",
    "d1 = scoping(tf.variable_scope, 'scope_vars', 'res', [1, 2, 3])\n",
    "d2 = scoping(tf.name_scope,     'scope_name', 'res', [1, 2, 3])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run([d1, d2]))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License: See LICENSE\n",
    "# Fit a straight line, of the form y=m*x+b\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Your dataset.\n",
    "'''\n",
    "xs = np.linspace(0.0, 8.0, 8000000) # 8-million features\n",
    "ys = 0.3*xs-0.8+np.random.normal(scale=0.25, size=len(xs)) # 8-million labels\n",
    "\n",
    "'''\n",
    "Initial guesses, which will be refined by TensorFlow.\n",
    "'''\n",
    "m_initial = -0.5 # Initial guesses\n",
    "b_initial =  1.0\n",
    "\n",
    "'''\n",
    "Define free variables to be solved.\n",
    "'''\n",
    "m = tf.Variable(m_initial) # Parameters\n",
    "b = tf.Variable(b_initial)\n",
    "\n",
    "'''\n",
    "Define placeholders for big data.\n",
    "'''\n",
    "_BATCH = 8 # Use only eight points at a time.\n",
    "xs_placeholder = tf.placeholder(tf.float32, [_BATCH])\n",
    "ys_placeholder = tf.placeholder(tf.float32, [_BATCH]) \n",
    "\n",
    "'''\n",
    "Define the error between the data and the model as a tensor (distributed computing).\n",
    "'''\n",
    "ys_model = m*xs_placeholder+b # Tensorflow knows this is a vector operation\n",
    "total_error = tf.reduce_sum((ys_placeholder-ys_model)**2) # Sum up every item in the vector\n",
    "\n",
    "'''\n",
    "Once cost function is defined, create gradient descent optimizer.\n",
    "'''\n",
    "optimizer_operation = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(total_error) # Does one step\n",
    "\n",
    "'''\n",
    "Create operator for initialization.\n",
    "'''\n",
    "initializer_operation = tf.global_variables_initializer()\n",
    "\n",
    "'''\n",
    "All calculations are done in a session.\n",
    "'''\n",
    "with tf.Session() as session:\n",
    "\n",
    "\tsession.run(initializer_operation) # Call operator\n",
    "\n",
    "\t_EPOCHS = 10000 # Number of \"sweeps\" across data\n",
    "\tfor iteration in range(_EPOCHS):\n",
    "\t\trandom_indices = np.random.randint(len(xs), size=_BATCH) # Randomly sample the data\n",
    "\t\tfeed = {\n",
    "\t\t\txs_placeholder: xs[random_indices],\n",
    "\t\t\tys_placeholder: ys[random_indices]\n",
    "\t\t}\n",
    "\t\tsession.run(optimizer_operation, feed_dict=feed) # Call operator\n",
    "\n",
    "\tslope, intercept = session.run((m, b)) # Call \"m\" and \"b\", which are operators\n",
    "\tprint('Slope:', slope, 'Intercept:', intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autograph.to_code(leapfrog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        #calculate potential and gradient at initial position\n",
    "        #assign current_grad_value to current_grad model \n",
    "        current_potential_value, current_grad_value = session.run([potential_model, grad_model])\n",
    "\n",
    "        tf.assign(current_grad, np.float64(current_grad_value))\n",
    "        #print('all obs times: ', session.run(t_obs))\n",
    "        for i in range(nobspoints):\n",
    "            t_current = t_obs[i]\n",
    "            deltat = t_current - t_previous\n",
    "            #print('current time: ', session.run(t_current))\n",
    "            session.run(tf.assign(dt, deltat))\n",
    "            new_x, new_v, new_grad, new_t, x_value, v_value = session.run(model_leap)\n",
    "            session.run(tf.assign(x[i], x_value))\n",
    "            session.run(tf.assign(v[i], v_value))\n",
    "            session.run(tf.assign(current_x, new_x))\n",
    "            session.run(tf.assign(current_v, new_v))\n",
    "            session.run(tf.assign(current_grad, new_grad))\n",
    "            session.run(tf.assign(t_previous, new_t + t_previous))\n",
    "            #print('current dt: ', session.run(dt))\n",
    "            time_step[i] = session.run(t_previous)\n",
    "            grads[i] = new_grad\n",
    "            #print('current x: ', session.run(current_x))\n",
    "            #print('current v: ', session.run(current_v))\n",
    "            #print('current grad: ', session.run(current_grad))\n",
    "        \"\"\"\n",
    "        #define loop over t_obs, inside loop assigns values to x, v\n",
    "        #with tf.name_scope(None, 'evolve'):\n",
    "            #print(nobspoints)\n",
    "            count = tf.scan(evolve, np.arange(nobspoints), \n",
    "                              initializer=(tf.constant(np.float64(0.0), dtype=np.float64), \n",
    "                                           tf.constant(np.float64(0.0), dtype=np.float64),\n",
    "                                           tf.constant(np.float64(0.0), dtype=np.float64),\n",
    "                                           tf.constant(np.float64(0.0), dtype=np.float64),\n",
    "                                           tf.constant(np.float64(0.0), dtype=np.float64),\n",
    "                                           tf.constant(np.float64(0.0), dtype=np.float64)))\n",
    "                                           #tf.constant(np.int32(0.0))])\n",
    "                                              #[tf.constant(np.int32(0.0))])\n",
    "            print(session.run(count))\n",
    "        #loop = tf.map_fn(evolve, iteration) #, initializer=tf.constant(np.float64(0.0)))\n",
    "\n",
    "        #session.run(loop)\n",
    "        with tf.variable_scope('Main', reuse=tf.AUTO_REUSE):\n",
    "        \"\"\"\n",
    "def counter_fn(counter):  # pylint: disable=unused-argument\n",
    "    return counter  < 10\n",
    "\n",
    "def evolve(params, i):\n",
    "    print(i)\n",
    "    _, _, _, t_previous, _, _ = params\n",
    "    with tf.variable_scope('Main', reuse=tf.AUTO_REUSE):\n",
    "        tcurrent = tf.get_variable(\"t_obs\", [1], dtype=np.float64)[i]\n",
    "        dt = tcurrent - t_previous \n",
    "        new_x, new_v, new_grad, new_t, x_value, v_value = model_leap\n",
    "\n",
    "    return new_x, new_v, new_grad, new_t, x_value, v_value\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
